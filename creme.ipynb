{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering\n",
    "\n",
    "I'll start by installing the Creme library. \n",
    "\n",
    "``pip install git+https://github.com/creme-ml/creme --upgrade``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing the packages that I need to train my models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import collections \n",
    "import datetime\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import compose\n",
    "from creme import feature_extraction\n",
    "from creme import linear_model\n",
    "from creme import metrics\n",
    "from creme import neighbors\n",
    "from creme import optim\n",
    "from creme import preprocessing\n",
    "from creme import stats\n",
    "from creme import stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this first function to parse the date and extract the number of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(x):\n",
    "    \"\"\"Extract features from the date.\"\"\"\n",
    "    import datetime\n",
    "    if not isinstance(x['date'], datetime.datetime):\n",
    "        x['date'] = datetime.datetime.strptime(x['date'], '%Y-%m-%d')\n",
    "    x['wday'] = x['date'].weekday()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_metadata`` allow me to extract the identifier of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(x):\n",
    "    key = x['id'].split('_')\n",
    "    x['item_id'] = f'{key[0]}_{key[1]}_{key[2]}'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I define the feature extraction pipeline. I use the module ``feature_extraction.TargetAgg`` to calculate the features on the target variable of the stream. I calculate rolling features on the target variable. I use the ``Shift`` module to easily compute features with a lag. I calculate rolling averages on the target variable with a lag of about 30 days. I also calculate the rolling average of the target variable as a function of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = compose.TransformerUnion(\n",
    "    compose.Select('wday'),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Mean()),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Var()),\n",
    "\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(3)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(15)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(20)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(30)),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Shift(30) | stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Shift(29) | stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Shift(28) | stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Shift(27) | stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Shift(26) | stats.RollingMean(1)),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(3)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(15)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(20)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(30)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will train two models per product, which represents **30490 * 2 models**. The first model with every product is a ``KNeighborsRegressor``. The second is a linear model. I noticed that these two models are complementary. I will select the best of the two models for each product as the model I will deploy in production.\n",
    "\n",
    "The code below allows me to declare my two pipelines, the first one dedicated to KNN and the second one to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init pipeline dedicated to KNN\n",
    "knn = (\n",
    "    compose.FuncTransformer(get_metadata) |\n",
    "    compose.FuncTransformer(extract_date) |\n",
    "    extract_features |\n",
    "    neighbors.KNeighborsRegressor(window_size=300, n_neighbors=30, p=2)\n",
    ")\n",
    "\n",
    "\n",
    "# Init pipeline dedicated to linear model\n",
    "lm = (\n",
    "    compose.FuncTransformer(get_metadata) |\n",
    "    compose.FuncTransformer(extract_date) |\n",
    "    extract_features |\n",
    "    linear_model.LinearRegression(optimizer=optim.SGD(0.00005), clip_gradient=1, intercept_lr=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below creates a copy of both pipelines for all products and store them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60980it [00:16, 3649.20it/s]\n",
      "100%|██████████| 30490/30490 [01:04<00:00, 475.75it/s]\n",
      "100%|██████████| 30490/30490 [00:58<00:00, 524.95it/s]\n"
     ]
    }
   ],
   "source": [
    "list_model = []\n",
    "\n",
    "X_y = stream.iter_csv('./data/sample_submission.csv', target_name='F8')\n",
    "\n",
    "for x, y in tqdm.tqdm(X_y, position=0):\n",
    "    \n",
    "    item_id = '_'.join(x['id'].split('_')[:5])\n",
    "\n",
    "    if item_id not in list_model:\n",
    "\n",
    "        list_model.append(item_id)\n",
    "        \n",
    "dict_knn = {item_id: copy.deepcopy(knn) for item_id in tqdm.tqdm(list_model, position=0)}\n",
    "dict_lm  = {item_id: copy.deepcopy(lm) for item_id in tqdm.tqdm(list_model, position=0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do a warm-up of all the models from a subset of the training set. To do this pre-training, I selected the last two months of the training set and saved it in csv format.I use Creme's ``stream.iter_csv`` module to iterate on the training dataset. The pipeline below consumes very little RAM memory because we load the data into the memory one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1829400it [2:41:38, 188.64it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "params = dict(target_name='y', converters={'y': int, 'id': str}, parse_dates= {'date': '%Y-%m-%d'})\n",
    "\n",
    "# Init streaming csv reader\n",
    "X_y = stream.iter_csv('./data/train_preprocessed.csv', **params)\n",
    "\n",
    "bar = tqdm.tqdm(X_y, position = 0)\n",
    "\n",
    "# Init online metrics:\n",
    "metric_knn = collections.defaultdict(lambda: metrics.MAE())\n",
    "metric_lm  = collections.defaultdict(lambda: metrics.MAE())\n",
    "\n",
    "for i, (x, y) in enumerate(bar):\n",
    "    \n",
    "    # Extract item id\n",
    "    item_id  = '_'.join(x['id'].split('_')[:5])\n",
    "    \n",
    "    # KNN\n",
    "    \n",
    "    # Evaluate performance of KNN\n",
    "    y_pred_knn = dict_knn[f'{item_id}'].predict_one(x)\n",
    "    \n",
    "    # Update metric of KNN\n",
    "    metric_knn[f'{item_id}'].update(y, y_pred_knn)\n",
    "    \n",
    "    # Fit KNN\n",
    "    dict_knn[f'{item_id}'].fit_one(x=x, y=y)\n",
    "    \n",
    "    # Linear Model\n",
    "    \n",
    "    # Evaluate performance of linear model\n",
    "    y_pred_lm  = dict_lm[f'{item_id}'].predict_one(x)\n",
    "    \n",
    "    # Update metric of linear model\n",
    "    metric_lm[f'{item_id}'].update(y, y_pred_lm)\n",
    "    \n",
    "    # Train linear model for 10 epochs on each training example\n",
    "    for _ in range(10):\n",
    "        dict_lm[f'{item_id}'].fit_one(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, I prefer to save the metrics I have calculated for all products. My goal is to compare for each product the performance of the KNN and the linear model. I will select the better of the two for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "scores_knn = {id: _.get() for id, _ in metric_knn.items()}\n",
    "\n",
    "scores_lm  = {id: _.get() for id, _ in metric_lm.items()}\n",
    "\n",
    "with open('scores_knn.json', 'w') as file:\n",
    "    \n",
    "    json.dump(scores_knn, file)\n",
    "\n",
    "with open('scores_lm.json', 'w') as file:\n",
    "    \n",
    "    json.dump(scores_lm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [00:00<00:00, 516387.22it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for item_id in tqdm.tqdm(scores_knn.keys()):\n",
    "    \n",
    "    score_knn = scores_knn[item_id]\n",
    "    \n",
    "    score_lm  = scores_lm[item_id]\n",
    "    \n",
    "    if score_knn < score_lm:\n",
    "        \n",
    "        scores[item_id] = score_knn\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores[item_id] = score_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save both KNN and linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "# Per item name warning\n",
    "with open('dict_knn_item.dill', 'wb') as file:\n",
    "    \n",
    "    dill.dump(dict_knn, file)\n",
    "    \n",
    "with open('dict_lm_item.dill', 'wb') as file:\n",
    "    \n",
    "    dill.dump(dict_lm, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load scores and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('scores_knn.json', 'rb') as file:\n",
    "    \n",
    "    scores_knn = json.load(file)\n",
    "\n",
    "with open('scores_lm.json', 'rb') as file:\n",
    "    \n",
    "    scores_lm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('dict_knn_item.dill', 'rb') as file:\n",
    "    \n",
    "    dict_knn = dill.load(file)\n",
    "    \n",
    "with open('dict_lm_item.dill', 'rb') as file:\n",
    "    \n",
    "    dict_lm = dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product, I choose the best model between KNNRegressor and linear model depending on the validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [00:00<00:00, 34529.31it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_model = {}\n",
    "\n",
    "for item_id in tqdm.tqdm(scores_knn.keys()):\n",
    "    \n",
    "    score_knn = scores_knn[item_id]\n",
    "    \n",
    "    score_lm  = scores_lm[item_id]\n",
    "    \n",
    "    if score_knn < score_lm:\n",
    "        \n",
    "        dict_model[item_id] = dict_knn[item_id]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dict_model[item_id] = dict_lm[item_id]\n",
    "        \n",
    "# Save selected models:\n",
    "with open('dict_model_item.dill', 'wb') as file:\n",
    "    \n",
    "    dill.dump(dict_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment of the model:\n",
    "\n",
    "**Now that all the models are pre-trained, I will be able to deploy the pipelines behind an API in a production environment. I will use the [Chantilly](https://github.com/creme-ml/chantilly) library to do so.**\n",
    "\n",
    "**[Chantilly](https://github.com/creme-ml/chantilly) is a project that aims to ease train Creme models when they are deployed. Chantilly is a minimalist API based on the Flask framework.** Chantilly allows to make predictions, train models and measure model performance in real time. It gives access to a dashboard.\n",
    "\n",
    "Chantilly is a library currently under development. For various reasons, I choose to extract the files from Chantilly that I'm interested in to realize this project. I chose to deploy my API with [Digital Ocean](https://www.digitalocean.com). You will be able to find the whole architecture of my API [here](https://github.com/raphaelsty/M5-Forecasting-Accuracy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To deploy my API, I followed the following steps:\n",
    "\n",
    "\n",
    "- I selected the server on Digital Ocean with the smallest configuration\n",
    "\n",
    "\n",
    "- Tutorial to initialize my server and firewall [here](https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-16-04)\n",
    "\n",
    "\n",
    "- Tutorial to install Anaconda on my server [here](https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-16-04)\n",
    "\n",
    "\n",
    "- Allow reading on port 8080 to be able to request my API ``sudo ufw allow 8080``\n",
    "\n",
    "\n",
    "- Installation of git ``sudo apt install git``\n",
    "\n",
    "\n",
    "- Clone my API on the server ``git clones https://github.com/raphaelsty/M5-Forecasting-Accuracy.git``\n",
    "\n",
    "\n",
    "- I installed the requirements of my repo ``pip install -r requirements.txt``.\n",
    "\n",
    "\n",
    "- I went to the repository I cloned and ran the following command to start my API:\n",
    "``waitress-serve --call 'app:create_app``.\n",
    "\n",
    "That's it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initialize my API with flavor regression (see Chantilly tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://creme-ml.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post(f'{url}/api/init', json= {'flavor': 'regression'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the flavor of my API, I upload all the models I've pre-trained. Each model has a name. This name is the name of the product. I have used dill to serialize the model before uploading it to my API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_model_item.dill', 'rb') as file:\n",
    "    dic_models = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3049 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for model_name, model in tqdm.tqdm(dic_models.items(), position=0):\n",
    "    r = requests.post(f'{url}/api/model/{model_name}', data=dill.dumps(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models are now deployed in production and available to make predictions. The models can also be updated on a daily basis. That's it.\n",
    "\n",
    "![](static/online_learning.png)\n",
    "\n",
    "**As you may have noticed, the philosophy of online learning allows to reduce the complexity of the deployment of a machine learning algorithm in production. Moreover, to update the model, we only have to make calls to the API. We don't need to re-train the model from scratch.** To maintain my models on a daily basis, I recommend setting up a script that queries the database that stores the sales made that day. This script would perform 30490 queries every day to update the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction by calling the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(f'{url}/api/predict', json={\n",
    "    'id': 1,\n",
    "    'model': 'HOBBIES_1_001_CA_1',\n",
    "    'features': {'date': '2016-05-23', 'id': 'HOBBIES_1_001_CA_1'}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update models with new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(f'{url}/api/learn', json={\n",
    "    'id': 1,\n",
    "    'model': 'HOBBIES_1_001_CA_1',\n",
    "    'ground_truth': 1,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
