{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by installing the cream library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install creme\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing the packages that I'm going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import compose\n",
    "from creme import feature_extraction\n",
    "from creme import neighbors\n",
    "from creme import metrics\n",
    "from creme import optim\n",
    "from creme import preprocessing\n",
    "from creme import stats\n",
    "from creme import stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this first function to parse the date and extract the number of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(x):\n",
    "    \"\"\"Extract features from the date.\"\"\"\n",
    "    import datetime\n",
    "    if not isinstance(x['date'], datetime.datetime):\n",
    "        x['date'] = datetime.datetime.strptime(x['date'], '%Y-%m-%d')\n",
    "    x['wday'] = x['date'].weekday()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_metadata`` allows you to extract the identifier of the product and the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(x):\n",
    "    key = x['id'].split('_')\n",
    "    x['cat_id'] = f'{key[0]}'\n",
    "    x['dept_id'] = f'{x[\"cat_id\"]}_{key[1]}'\n",
    "    x['item_id'] = f'{x[\"cat_id\"]}_{x[\"dept_id\"]}_{key[2]}'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I define the feature extraction pipeline. I use the module ``feature_extraction.TargetAgg`` to calculate the features on the target variable of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = compose.TransformerUnion(\n",
    "    compose.Select('wday'),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Mean()),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.Var()),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(1)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(30)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(15)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by=['item_id'], how=stats.RollingMean(3)),\n",
    "    \n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(30)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(15)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(3)),\n",
    "    feature_extraction.TargetAgg(by=['wday'], how=stats.RollingMean(1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I define the global pipeline I want to deploy in production. The pipeline is composed of:\n",
    "\n",
    "- Extraction of the product identifier.\n",
    "\n",
    "- Extraction of the day number of the date $\\in$ {1, 2, ..7}. \n",
    "\n",
    "- Computation of the features.\n",
    "\n",
    "- Standard scaler that centers and reduces the value of features.\n",
    "\n",
    "- Model declaration ``neighbors.KNeighborsRegressor``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_metadata) |\n",
    "    compose.FuncTransformer(extract_date) |\n",
    "    extract_features |\n",
    "    preprocessing.StandardScaler() |\n",
    "    neighbors.KNeighborsRegressor(window_size=30, n_neighbors=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to create one template per product and per store. The piece of code below create a copy of the pipeline for all product/store pairs and store them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model = []\n",
    "\n",
    "X_y = stream.iter_csv('./data/sample_submission.csv', target_name='F8')\n",
    "\n",
    "for x, y in tqdm.tqdm(X_y, position=0):\n",
    "    \n",
    "    item_id = '_'.join(x['id'].split('_')[:5])\n",
    "    \n",
    "    if item_id not in list_model:\n",
    "    \n",
    "        list_model.append(item_id)\n",
    "        \n",
    "dic_models = {item_id: copy.deepcopy(model) for item_id in tqdm.tqdm(list_model, position=0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make a warm-up of all the models from a subset of the training set. To do this pre-training, I selected the last two months of the training set and saved it in csv format.I use Creme's ``stream.iter_csv`` module to iterate on the training dataset. The pipeline below consumes very little RAM memory because we load the data into the memory one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "params = dict(\n",
    "    target_name='y', \n",
    "    converters={\n",
    "        'y': int, \n",
    "        'id': str,\n",
    "    },\n",
    "    parse_dates= {'date': '%Y-%m-%d'},\n",
    ")\n",
    "\n",
    "X_y = stream.iter_csv('./data/train.csv', **params)\n",
    "\n",
    "bar = tqdm.tqdm(X_y, position = 0)\n",
    "\n",
    "metric = metrics.Rolling(metrics.MAE(), 300000)\n",
    "\n",
    "for i, (x, y) in enumerate(bar):\n",
    "    \n",
    "    item_id = '_'.join(x['id'].split('_')[:5])\n",
    "\n",
    "    # Predict:\n",
    "    y_pred = dic_models[item_id].predict_one(x)\n",
    "\n",
    "    # Update the model:\n",
    "    dic_models[item_id].fit_one(x=x, y=y)\n",
    "\n",
    "    # Update the metric:\n",
    "    metric = metric.update(y, y_pred)\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "\n",
    "        # Update tqdm progress bar every 4000 iterations.\n",
    "        bar.set_description(f'MAE: {metric.get():4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment of the model:\n",
    "\n",
    "Now that all the models are pre-trained, I will be able to deploy the pipelines behind an API in a production environment. I will use the [Chantilly](https://github.com/creme-ml/chantilly) library to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install git+https://github.com/creme-ml/chantilly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing Chantilly, I start the chantilly instance with the bash command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "chantilly run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to associate the regression flavor with the Chantilly API. Chantilly uses this flavor to select the appropriate metrics (MAE, MSE and SMAPE). Finally, I deploy all my models in production. Each model is identifiable by its name which is composed of the product identifier and the store identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://127.0.0.1:5000/api/init', json= {'flavor': 'regression'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the whipped cream API, I upload all the templates I've pre-trained. Each model has a name. This name is composed of the product and store ID. I use dill to serialize the model before uploading it to my API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "for model_name, model in dic_models.items():\n",
    "    \n",
    "    r = requests.post('http://localhost:5000/api/model/{}'.format(model_name), data=dill.dumps(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models are now deployed in production and available to make predictions. The models can also be updated on a daily basis. That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/online_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you may have noticed, the philosophy of online learning allows to reduce the complexity of the deployment of a machine learning algorithm in production. Moreover, to update the model, we only have to make calls to the API. We don't need to re-train the model from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction by calling the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\n",
    "    'id': 1,\n",
    "    'model': 'HOBBIES_1_001_CA_1',\n",
    "    'features': {'date': '2020-04-30', 'id': 'HOBBIES_1_001_CA_1'}\n",
    "}\n",
    "\n",
    "r = requests.post('http://localhost:5000/api/predict', json=json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update models with new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://localhost:5000/api/learn', json={\n",
    "    'id': 1,\n",
    "    'model': 'HOBBIES_1_001_CA_1',\n",
    "    'ground_truth': 2,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
